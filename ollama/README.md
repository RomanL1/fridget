# LLM runtime setup instructions

## Run the project

The LLM runtime is composed as a docker container locally. Run the [Backend API](../backend/README.md) project to compose this container.

A model will be downloaded automatically upon the first request. These are the models to choose from: https://ollama.com/search
